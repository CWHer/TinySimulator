# TinyML Simulator

## Description

![timeline](README.assets/timeline.png)

This is a simple simulator that simulates neural network behavior in typical TinyML devices. :memo:

More specifically, we assume the underlying device with <u>two levels of hierarchical memory</u>. The <u>slow memory</u> (e.g., Flash) CAN NOT be directly used in computation (we must load them to <u>fast memory</u> first), and it supports <u>full-duplex loading & storing</u>.

Given operators/device specifications and execution plan, the simulator yields detailed memory usage curves and utilization of functional components (illustrated in the above figure) to better evaluate the execution plan.

In the following section, we briefly describe some important classes.



## Important Notes

1. As our application scenario is TinyML, we assume each operator (partition) can saturate the underlying accelerator. Thus, we DO NOT consider the parallelism of operators.

2. We assume the underlying machine with two levels of hierarchical memory. The slow memory (e.g., NVMe) CAN NOT be directly used in computation, but it supports full-duplex loading & storing.

3. We assume fast memory allocation & copy is immediate without any time elapsing.

4. For simplicity, we DO NOT consider technics like re-materialization. (TODO)

5. We assume pruning is output channel-wise. Thus, if an operator is not pruned entirely, it can pass gradient to its predecessors.

6. We support operator-level (channel-wise) parallelism to further reduce the memory footprint.



### Operator

There are two kinds of relations between channels, `AllToAll` (e.g., linear) and `OneToOne` (e.g., batch norm).

- Forward phase

  $$
  X^\prime = f(w,X)
  $$

  - All-to-all: `(pred_)output(all) + param(out_ch) -> output(out_ch)`

  - One-to-one: `(pred_)output(ch) + param(out_ch) -> output(ch)`

  `(pred_)output(ch) -> input(ch)`

- Backward phase

  $$
  \frac{\partial l}{\partial w} = \frac{\partial l}{\partial X^\prime} \times \frac{\partial f}{\partial w}\\
  \frac{\partial l}{\partial X} = \frac{\partial l}{\partial X^\prime} \times \frac{\partial f}{\partial X}
  $$

  - All-to-all

    `(succ_)pass_grad(ch) + input(all) -> grad(ch)`

    `(succ_)pass_grad(ch) + param(out_ch) -> pass_grad(all)`

  - One-to-one

    `(succ_)pass_grad(ch) + input(ch) -> grad(ch)`

    `(succ_)pass_grad(ch) + param(out_ch) -> pass_grad(ch)`

- Optimize phase

  $$
  w^\prime = w- \text{lr} \times \mathrm dw
  $$

```python
class Operator:
    forward_memory_peak: float     # extra memory usage during forward (i.e., DO NOT include input, param, ...)
    forward_time_elapsed: float    # time elapsed during forward
    backward_memory_peak: float    # extra memory usage during backward 
    backward_time_elapsed: float   # time elapsed during backward
    optimize_time_elapsed: float   # time elapsed during optimize

    param_size: float        # size of parameters data
    input_size: float        # size of input data
    output_size: float       # size of output data
    grad_size: float         # size of gradient data
    pass_grad_size: float    # size of passing gradient data

    num_input_channels: int    # number of input channels
    num_output_channels: int   # number of output channels

    pred_ops: List[Operator] = None  # predecessor operators
    succ_ops: List[Operator] = None  # successor operators

    output_channel_accuracy: List[float] = None  # accuracy loss of each output channel

    param_locations: List[MemoryType] = None        # parameters data
    input_locations: List[MemoryType] = None        # input data 
    output_locations: List[MemoryType] = None       # output data
    grad_locations: List[MemoryType] = None         # gradient data
    pass_grad_locations: List[MemoryType] = None    # passing gradient data
```

### Device

```python
class RunTime:
    memory_limit: float  # fast memory limit
    cross_level_bandwidth_read: float
    cross_level_bandwidth_write: float
    target_accuracy: float  # for pruning

class MemoryType(Enum):
    FAST = 1        # in fast memory
    SLOW = 2        # in slow memory
    NONE = 3        # not initialized
    RUNNING = 5     # occupied by operator
```

### Decision

```python
class DecisionType(Enum):
    LOAD = 1        # load data from slow memory
    STORE = 2       # store data to slow memory
    ALLOCATE = 3    # allocate fast memory
    PURGE = 4       # purge fast memory (NOTE: generated by the simulator)
    FORWARD = 6     # run operator forward
    BACKWARD = 7    # run operator backward
    OPTIMIZE = 8    # optimize the parameters
    PRUNE = 9       # prune along output channels (in backward phase)
    COMMIT = 10     # commit the computation (NOTE: generated by the simulator)
    
class Decision:
    wall_time: float  # when this decision happens
    decision_type: DecisionType

    operator: Operator  # apply to which operator
    # which memory block to load/store/allocate
    memory_block: MemoryBlockType
    channel_ids: List[int]  # apply to which channels
```



## Getting Started

**NOTE**: Running these programs will generate timeline figure of the model execution.

```bash
# run demo
cd examples
python demo.py
# run mcunet-320kb
cd examples/mcunet-320kb
python main_plan.py
```

### Simple Demo

We provide a simple demo in `examples/demo.py`.

The steps for generating this simple execution plan can be summarized as follows:

- In forward phase, for each operator, 

  1. load/allocate input data (dataset is stored in slow memory)

  2. load parameters from slow memory

  3. allocate memory for output data

  4. execute forward

- Optional: prune some channels

- In backward phase, for each operator,

  1. allocate memory for gradient data

  2. allocate memory for passing gradient data

  3. execute backward & optimize for not pruned channels

- Finally, store all parameters to slow memory



## Files

```
│  README.md
├─examples
│  │  demo.py
│  └─mcunet-320kb
│       main_plan.py
│       network_config.py
│       op_wrappers.py
└─src
    operator_class.py
    runtime_class.py
    simulator.py
    solution_class.py
    utils.py
    __init__.py
```



## TODO

- [ ] Remove explicit issue time of decision and let simulator automatically issue them.

  To achieve this, one possible way is to make decisions connected in a directed acyclic graph. Otherwise, when the simulator statically generates `purge`/`commit` decisions and it may not be able to properly insert them into the current execution plan.  

  Besides, all the statical analyses need to be refactored so they could incorporate with automatic issue mechanism.
